# -*- coding: utf-8 -*-
"""DataLab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nrIkfJLu989in_UiaY2wh_BhFwGPnIDG

# Projeto: Atrasos de Voo - Laboratoria (Débora Vasconcellos e Mayara Alves)
"""

import plotly.express as px
from scipy.stats import shapiro
from sklearn.datasets import make_regression
from sklearn.linear_model import LinearRegression


import scipy.stats as stats
from statsmodels.formula.api import ols


#limpeza e manipucação de dados
import pandas as pd
import numpy as np

#visualização dos dados
import matplotlib.pyplot as plt
import seaborn as sns

#machine learning
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,balanced_accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split,learning_curve, KFold
from sklearn.metrics import confusion_matrix,roc_curve, roc_auc_score, log_loss
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
from imblearn.over_sampling import ADASYN

import statsmodels.api as sm
import statsmodels.formula.api as smf

df = pd.read_csv("flights_202301.csv")

"""## Informações Gerais - Tabela flights_202301"""

#mstrando o dataframe#
df

"""## Verificando o tipo dos dados"""

#mostrando os tipos de dados das colunas##
df.info()

"""## Principais estatísticas"""

##mostrando as principais estatísticas das colunas##
df.describe()

# Filtrar valores acima de 2400 na coluna DEP_DELAY
result = df[df['DEP_DELAY'] > 2400]

# Ajustar opções de exibição para mostrar todas as colunas
pd.set_option('display.max_columns', None)  # Mostrar todas as colunas
pd.set_option('display.expand_frame_repr', False)  # Não quebrar linhas

# Exibir todas as colunas do resultado
print(result)

df.info()

"""## Verificando se encontramos valores nulos"""

##somando os valores nulos##
df.isnull().sum()

"""## Selecionando colunas com nulos que iremos remover"""

#selecionando colunas que iremos remover os nulos#
colunas_nulos_remover = ["DEP_TIME", "DEP_DELAY", "TAXI_OUT", "WHEELS_OFF","TAXI_IN", "ARR_TIME", "ARR_DELAY","CRS_ELAPSED_TIME","ELAPSED_TIME","AIR_TIME"]
df.dropna(subset=colunas_nulos_remover, inplace=True)

"""## Verificando se os nulos foram removidos"""

##verificando se os nulos das variaveis selecionadas saíram##
df.isnull().sum()

"""## Alterado os valores nulos restantes para 0"""

##alterando os para nulos para 0##
df.fillna(0,inplace=True)

"""## Confirmando se os nossos dados estão sem os nulos"""

#efetuando a soma novamente, para verificar se os nulos saíram#
df.isnull().sum()

"""## Verificando se encontramos valores duplicados"""

##verificando se há duplicados##
df.duplicated().sum()

"""## Consulta para verificar a presença de dados discrepantes nas variáveis categóricas



"""

num_nomes_unicos = df['ORIGIN_CITY'].nunique()
nomes_unicos = df['ORIGIN_CITY'].unique()

# Imprimir o número de nomes únicos
print(f"Número de nomes únicos em ORIGIN_CITY: {num_nomes_unicos}")

# Imprimir os nomes únicos
print("Nomes únicos em ORIGIN_CITY:")
print(nomes_unicos)

num_nomes_unicos = df['DEST_CITY'].nunique()
nomes_unicos = df['DEST_CITY'].unique()

# Imprimir o número de nomes únicos
print(f"Número de nomes únicos em DEST_CITY: {num_nomes_unicos}")

# Imprimir os nomes únicos
print("Nomes únicos em DEST_CITY:")
print(nomes_unicos)

"""## Histrograma para visualização das varíaveis numéricas"""

#Visualizando Histogramas
import plotly.express as px

def plot_histogram(dataframe, column, title):
    fig = px.histogram(dataframe, x=column, title=title)
    fig.show()

# Lista de colunas e títulos correspondentes
columns_titles = [
    ("DEP_DELAY", "Distribuição de voos pela diferença em minutos entre o horário de partida programado e o real"),
    ("ARR_DELAY", "Distribuição de voos pela diferença em minutos entre o horário de chegada e o real"),
    ("CRS_ELAPSED_TIME", "Distribuição do tempo total de voo decorrido em minutos registrado no CRS"),
    ("DELAY_DUE_WEATHER", "Distribuição de atraso climático em minutos"),
    ("DELAY_DUE_CARRIER", "Distribuição de voos por atraso da operadora em minutos"),
    ("DELAY_DUE_NAS", "Distribuição do atraso do Sistema Aéreo Nacional em Minutos"),
    ("DELAY_DUE_SECURITY", "Distribuição de atraso de segurança em minutos"),
    ("DELAY_DUE_LATE_AIRCRAFT", "Distribuição de atraso de aeronaves atrasadas em minutos")
]

# Gerar os histogramas
for column, title in columns_titles:
    plot_histogram(df, column, title)

"""## Criando variável categórica para horário de voo"""

# Função para categorizar os horários
def categorizar_horario(horario):

    if 0 <= horario < 600:
        return 'Late Night'
    elif 600 <= horario < 1200:
        return 'Morning'
    elif 1200 <= horario < 1800:
        return 'Afternoon'
    elif 1800 <= horario <= 2400:
        return 'Evening'
    else:
        return 'Invalid Hour'  # Para garantir que todos os horários sejam cobertos

# Aplicar a função para categorizar os horários de partida e chegada previstos
df['CAT_CRS_DEP_TIME'] = df['CRS_DEP_TIME'].apply(categorizar_horario)
df['CAT_CRS_ARR_TIME'] = df['CRS_ARR_TIME'].apply(categorizar_horario)

# Verificar as mudanças
print(df[['CRS_DEP_TIME', 'CAT_CRS_DEP_TIME', 'CRS_ARR_TIME', 'CAT_CRS_ARR_TIME']].head())

"""## Formatando as colunas selecionadas para o formato de hora"""

# Listar as colunas que precisam ser formatadas
colunas_de_tempo = [
    'CRS_DEP_TIME',
    'DEP_TIME',
    'WHEELS_OFF',
    'WHEELS_ON',
    'CRS_ARR_TIME',
    'ARR_TIME'
]

# Função para formatar o tempo de hhmm para hh:mm
def formatar_tempo(valor):
    if pd.isna(valor) or valor == '':
        return valor  # Retorna o valor original
    valor_str = str(int(valor)).zfill(4)  # Converte para string
    if valor_str == '2400':
        return '00:00'  # Caso especial: transforma "2400" em "00:00"
    return f"{valor_str[:2]}:{valor_str[2:]}"  # Formata como hh:mm

# Aplicando a formatação para cada coluna de tempo
for coluna in colunas_de_tempo:
    df[coluna] = df[coluna].apply(formatar_tempo)

# Verificando as mudanças
print(df[colunas_de_tempo].head())

"""## Formatando as colunas para INT64"""

# Listar as colunas que precisam ser formatadas para int64
colunas_para_int = [
    'DEP_DELAY',
    'ARR_DELAY',
    'CRS_ELAPSED_TIME',
    'ELAPSED_TIME',
    'AIR_TIME',
    'DELAY_DUE_CARRIER',
    'DELAY_DUE_WEATHER',
    'DELAY_DUE_NAS',
    'DELAY_DUE_SECURITY',
    'DELAY_DUE_LATE_AIRCRAFT'
]

# Converter as colunas para int64
for coluna in colunas_para_int:
    df[coluna] = df[coluna].astype('Int64')

# Verificar as mudanças
print(df[colunas_para_int].dtypes)
print(df[colunas_para_int].head())

"""## Correlação das varíaveis"""

# Selecionar apenas colunas numéricas para cálculo de correlação
numeric_df = df.select_dtypes(include=[np.number])

# Gerar a matriz de correlação
corr = numeric_df.corr().round(2)

# Remover colunas específicas da matriz de correlação
colunas_para_remover = ['CANCELLED', 'FL_YEAR', 'FL_MONTH', 'FL_DAY','DIVERTED']
corr = corr.drop(columns=colunas_para_remover, index=colunas_para_remover)

# Código para remover o triângulo superior
mask = np.triu(np.ones_like(corr, dtype=bool))

# Configure a figura do matplotlib
f, ax = plt.subplots(figsize=(11, 9))

# Gere um mapa de cores divergente personalizado
cmap = sns.diverging_palette(230, 20, as_cmap=True)

# Desenhe o mapa de calor com a máscara e a proporção correta e anote os valores
sns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot=True, fmt=".2f")

plt.show()

"""## Criando nova variável dia da semana"""

# Converter a coluna FL_DATE para o formato de data
df['FL_DATE'] = pd.to_datetime(df['FL_DATE'])

# Criar a nova coluna com o dia da semana
df['DAY_OF_WEEK'] = df['FL_DATE'].dt.day_name()

#Verificar mudanças
print(df)

"""## Unificando as tabelas"""

import pandas as pd

# Leitura dos dicionários de códigos
airline_code_dict = pd.read_csv("AIRLINE_CODE_DICTIONARY.csv")
dot_code_dict = pd.read_csv("DOT_CODE_DICTIONARY.csv")

# Renomear as colunas 'code' nos dicionários
airline_code_dict = airline_code_dict.rename(columns={'Code': 'AIRLINE_CODE', 'Description': 'AIRLINE_NAME'})
dot_code_dict = dot_code_dict.rename(columns={'Code': 'DOT_CODE', 'Description': 'DOT_NAME'})
# Verificar as primeiras linhas para entender a estrutura
print(airline_code_dict.head())
print(dot_code_dict.head())
# Realizar a mesclagem (merge) com base nas colunas correspondentes
# Mesclar com o dicionário de AIRLINE_CODE
df = df.merge(airline_code_dict, how='left', on='AIRLINE_CODE')
# Mesclar com o dicionário de DOT_CODE
df = df.merge(dot_code_dict, how='left', on='DOT_CODE')
# Função para determinar a principal causa de atraso com base nos somatórios
def principal_causa_atraso(row):
    causas = {
        'Carrier': row['DELAY_DUE_CARRIER'],
        'Weather': row['DELAY_DUE_WEATHER'],
        'National Airspace System': row['DELAY_DUE_NAS'],
        'Security': row['DELAY_DUE_SECURITY'],
        'Due delay': row['DELAY_DUE_LATE_AIRCRAFT']
    }
    if all(value == 0 for value in causas.values()):
        return 'Without delay'
    principal_causa = max(causas, key=causas.get)
    return principal_causa
# Aplicar a função para criar a nova coluna
df['MAIN_CAUSE_DELAY'] = df.apply(principal_causa_atraso, axis=1)
# Criar uma nova coluna binária indicando se o voo teve algum atraso
df['DELAY_FLAG'] = df.apply(lambda row: 0 if row['MAIN_CAUSE_DELAY'] == 'Without delay' else 1, axis=1)
# Criar a nova coluna 'FLIGHT_ROUTE'
df['FLIGHT_ROUTE'] = df['ORIGIN_CITY'] + ' - ' + df['DEST_CITY']
# Selecionar as colunas desejadas
# Incluindo as colunas originais e adicionando as colunas com os nomes dos códigos
colunas_selecionadas = ['FL_DATE', 'AIRLINE_CODE', 'AIRLINE_NAME', 'DOT_CODE', 'DOT_NAME', 'FL_NUMBER', 'ORIGIN', 'ORIGIN_CITY', 'DEST', 'DEST_CITY',
                        'CRS_DEP_TIME', 'DEP_TIME', 'DEP_DELAY', 'CRS_ARR_TIME', 'ARR_TIME',  'ARR_DELAY',
                        'CRS_ELAPSED_TIME', 'ELAPSED_TIME', 'AIR_TIME', 'DISTANCE', 'DELAY_DUE_CARRIER', 'DELAY_DUE_WEATHER', 'DELAY_DUE_NAS',
                        'DELAY_DUE_SECURITY', 'DELAY_DUE_LATE_AIRCRAFT', 'FL_YEAR', 'FL_MONTH', 'FL_DAY', 'MAIN_CAUSE_DELAY', 'DELAY_FLAG', 'FLIGHT_ROUTE','CAT_CRS_DEP_TIME','CAT_CRS_ARR_TIME','DAY_OF_WEEK']
# Ajustar a lista acima conforme necessário
df = df[colunas_selecionadas]
# Verificar as mudanças
print(df.head())

"""## Verificando a unificação"""

df

"""##Criando a variável HAS_DELAY para atrasos"""

df['HAS_DELAY'] = np.where(df['DEP_DELAY'] >= 10, 1, 0)

"""## Calculando a média de atraso por dia da semana na partida"""

# Agrupe pelos dias da semana e calcule a média dos atrasos de partida
avg_delay_by_day = df.groupby('DAY_OF_WEEK')['DEP_DELAY'].mean().sort_values(ascending=False)

# Exiba os dias da semana com os maiores atrasos médios
print(avg_delay_by_day)

# Calculando a média de atraso de partida por dia da semana
avg_delay_by_day = df.groupby('DAY_OF_WEEK')['DEP_DELAY'].mean().sort_values(ascending=False)

# Importando as bibliotecas necessárias
import matplotlib.pyplot as plt
import seaborn as sns

# Definindo a ordem dos dias da semana para plotagem ordenada
order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']

# Criando o gráfico de barras
plt.figure(figsize=(10, 6))
ax = sns.barplot(x=avg_delay_by_day.index, y=avg_delay_by_day.values, order=order, color='blue')
plt.title('Average Arrival Delay by Day of the Week')
plt.xlabel('Day week')
plt.ylabel('Atraso Médio de Chegada (minutos)')
plt.xticks(rotation=45)

# Adicionando legendas em cima das barras
for p in ax.patches:
    ax.annotate(f'{p.get_height():.1f}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),
                textcoords='offset points')

plt.tight_layout()
plt.show()

"""## Calculando a média de atraso por dia da semana na chegada"""

# Agrupe pelos dias da semana e calcule a média dos atrasos de chegada
avg_delay_by_day = df.groupby('DAY_OF_WEEK')['ARR_DELAY'].mean().sort_values(ascending=False)

# Exiba os dias da semana com os maiores atrasos médios
print(avg_delay_by_day)

# Calculando a média de atraso de chegada por dia da semana
avg_delay_by_day = df.groupby('DAY_OF_WEEK')['ARR_DELAY'].mean().sort_values(ascending=False)

# Importando as bibliotecas necessárias
import matplotlib.pyplot as plt
import seaborn as sns

# Definindo a ordem dos dias da semana para plotagem ordenada
order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']

# Criando o gráfico de barras
plt.figure(figsize=(10, 6))
ax = sns.barplot(x=avg_delay_by_day.index, y=avg_delay_by_day.values, order=order, color='blue')
plt.title('Average Arrival Delay by Day of the Week')
plt.xlabel('Day of the week')
plt.ylabel('Average Arrival Delay (minutes)')
plt.xticks(rotation=45)

# Adicionando legendas em cima das barras
for p in ax.patches:
    ax.annotate(f'{p.get_height():.1f}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),
                textcoords='offset points')

plt.tight_layout()
plt.show()

"""## Porcentagem e média de atrasos na partida"""

flight_counts = df['CAT_CRS_DEP_TIME'].value_counts()
average_delays = df.groupby('CAT_CRS_DEP_TIME')['DEP_DELAY'].mean()

print("Contagem de voos por segmento:")
print(flight_counts)

print("\nMédia de atrasos por segmento:")
print(average_delays)

"""## Porcentagem e média de atrasos na chegada"""

flight_counts = df['CAT_CRS_ARR_TIME'].value_counts()
flight_percentages = (flight_counts / flight_counts.sum()) * 100
average_delays = df.groupby('CAT_CRS_ARR_TIME')['DEP_DELAY'].mean()

print("Porcentagem de voos por segmento:")
print(flight_percentages)

print("\nMédia de atrasos por segmento:")
print(average_delays)

"""##Boxplot para visualição dos dados"""

sns.set(rc = {'figure.figsize':(20,8)})
fig = sns.boxplot(x="CAT_CRS_DEP_TIME", y='DEP_DELAY',data=df)
plt.xticks(rotation=90);

sns.set(rc = {'figure.figsize':(20,8)})
fig = sns.boxplot(x="MAIN_CAUSE_DELAY", y='DEP_DELAY',data=df)
plt.xticks(rotation=90);

sns.set(rc = {'figure.figsize':(20,8)})
fig = sns.boxplot(x="CAT_CRS_ARR_TIME", y='ARR_DELAY',data=df)
plt.xticks(rotation=90);

sns.set(rc = {'figure.figsize':(20,8)})
fig = sns.boxplot(x="MAIN_CAUSE_DELAY", y='ARR_DELAY',data=df)
plt.xticks(rotation=90);

"""##Passando as variáveis para categórica ordinal"""

# Dicionário de mapeamento
mapping = {
    'Late Night': 0,
    'Morning': 1,
    'Afternoon': 2,
    'Evening': 3
}

# Criando novas colunas numéricas no DataFrame original
df['NUM_CRS_DEP_TIME'] = df['CAT_CRS_DEP_TIME'].map(mapping)
df['NUM_CRS_ARR_TIME'] = df['CAT_CRS_ARR_TIME'].map(mapping)

# Exibindo o DataFrame atualizado com todas as colunas
print(df)

"""## Verificando se a tabela está com todas as varíaveis

"""

df

# Calculando a proporção de atrasos em cada turno de partida do voo
proporcoes_atraso = df.groupby('MAIN_CAUSE_DELAY')['HAS_DELAY'].mean()

# Calculando a proporção total de atrasos
proporcao_total_atraso = df['HAS_DELAY'].mean()

# Calculando o risco relativo para cada categoria em relação à proporção total de atrasos
risco_relativo = proporcoes_atraso / proporcao_total_atraso


print("\nRisco relativo por Turno de partida do voo em relação à proporção total de atrasos:")
print(risco_relativo)

"""### Risco relativo | CRS_ELAPSED_TIME"""

# Dividindo a variável CRS_ELAPSED_TIME em quartis
df['CRS_ELAPSED_TIME_QUARTILE'] = pd.qcut(df['CRS_ELAPSED_TIME'], 4, labels=False)

# Calculando a proporção de atrasos em cada quartil
proporcoes_atraso = df.groupby('CRS_ELAPSED_TIME_QUARTILE')['HAS_DELAY'].mean()

# Calculando a proporção total de atrasos
proporcao_total_atraso = df['HAS_DELAY'].mean()

# Calculando o risco relativo para cada quartil em relação à proporção total de atrasos
risco_relativo = proporcoes_atraso / proporcao_total_atraso

print("Proporções de atraso por quartil:")
print(proporcoes_atraso)
print("\nRisco relativo por quartil em relação à proporção total de atrasos:")
print(risco_relativo)

# Calculando a proporção de atrasos em cada quartil
proporcoes_atraso = df.groupby('DAY_OF_WEEK')['HAS_DELAY'].mean()

# Calculando a proporção total de atrasos
proporcao_total_atraso = df['HAS_DELAY'].mean()

# Calculando o risco relativo para cada quartil em relação à proporção total de atrasos
risco_relativo = proporcoes_atraso / proporcao_total_atraso


print("\nRisco relativo por Dia da Semana em relação à proporção total de atrasos:")
print(risco_relativo)

"""### Risco relativo | ELAPSED_TIME"""

# Dividindo a variável CRS_ELAPSED_TIME em quartis
df['ELAPSED_TIME_QUARTILE'] = pd.qcut(df['ELAPSED_TIME'], 4, labels=False)

# Calculando a proporção de atrasos (DELAY_FLAG == 1) em cada quartil
proporcoes_atraso = df.groupby('ELAPSED_TIME_QUARTILE')['HAS_DELAY'].mean()

# Calculando a proporção total de atrasos
proporcao_total_atraso = df['HAS_DELAY'].mean()

# Calculando o risco relativo para cada quartil em relação à proporção total de atrasos
risco_relativo = proporcoes_atraso / proporcao_total_atraso

print("Proporções de atraso por quartil:")
print(proporcoes_atraso)
print("\nRisco relativo por quartil em relação à proporção total de atrasos:")
print(risco_relativo)

"""### Risco relativo | AIR_TIME"""

# Dividindo a variável CRS_ELAPSED_TIME em quartis
df['AIR_TIME_QUARTILE'] = pd.qcut(df['AIR_TIME'], 4, labels=False)

# Calculando a proporção de atrasos em cada quartil
proporcoes_atraso = df.groupby('AIR_TIME_QUARTILE')['HAS_DELAY'].mean()

# Calculando a proporção total de atrasos
proporcao_total_atraso = df['HAS_DELAY'].mean()

# Calculando o risco relativo para cada quartil em relação à proporção total de atrasos
risco_relativo = proporcoes_atraso / proporcao_total_atraso

print("Proporções de atraso por quartil:")
print(proporcoes_atraso)
print("\nRisco relativo por quartil em relação à proporção total de atrasos:")
print(risco_relativo)

"""### Risco relativo | DISTANCE"""

# Dividindo a variável CRS_ELAPSED_TIME em quartis
df['DISTANCE_QUARTILE'] = pd.qcut(df['DISTANCE'], 4, labels=False)

# Calculando a proporção de atrasos (DELAY_FLAG == 1) em cada quartil
proporcoes_atraso = df.groupby('DISTANCE_QUARTILE')['HAS_DELAY'].mean()

# Calculando a proporção total de atrasos
proporcao_total_atraso = df['HAS_DELAY'].mean()

# Calculando o risco relativo para cada quartil em relação à proporção total de atrasos
risco_relativo = proporcoes_atraso / proporcao_total_atraso

print("Proporções de atraso por quartil:")
print(proporcoes_atraso)
print("\nRisco relativo por quartil em relação à proporção total de atrasos:")
print(risco_relativo)

"""## Risco relativo | DEP_TIME"""

# Calculando a proporção de atrasos em cada turno de partida do voo
proporcoes_atraso = df.groupby('CAT_CRS_DEP_TIME')['HAS_DELAY'].mean()

# Calculando a proporção total de atrasos
proporcao_total_atraso = df['HAS_DELAY'].mean()

# Calculando o risco relativo para cada categoria em relação à proporção total de atrasos
risco_relativo = proporcoes_atraso / proporcao_total_atraso


print("\nRisco relativo por Turno de partida do voo em relação à proporção total de atrasos:")
print(risco_relativo)

"""## Risco relativo | ARR_TIME"""

# Calculando a proporção de atrasos em cada turno de partida do voo
proporcoes_atraso = df.groupby('CAT_CRS_ARR_TIME')['HAS_DELAY'].mean()

# Calculando a proporção total de atrasos
proporcao_total_atraso = df['HAS_DELAY'].mean()

# Calculando o risco relativo para cada categoria em relação à proporção total de atrasos
risco_relativo = proporcoes_atraso / proporcao_total_atraso


print("\nRisco relativo por Turno de partida do voo em relação à proporção total de atrasos:")
print(risco_relativo)

"""## Criando a variável binaria HAS_DELAY_ARR para calcular o risco relativo"""

# Criar a variável binária para ARR_DELAY e utilizando para o cálculo do Risco Relativo de atrasos na chegada
df['HAS_DELAY_ARR'] = np.where(df['ARR_DELAY'] >= 10, 1, 0)

# Dividindo a variável DISTANCE em quartis
df['DISTANCE_QUARTILE'] = pd.qcut(df['DISTANCE'], 4, labels=False)

# Calculando a proporção de atrasos (HAS_DELAY_ARR == 1) em cada quartil
proporcoes_atraso = df.groupby('DISTANCE_QUARTILE')['HAS_DELAY_ARR'].mean()

# Calculando a proporção total de atrasos
proporcao_total_atraso = df['HAS_DELAY_ARR'].mean()

# Calculando o risco relativo para cada quartil em relação à proporção total de atrasos
risco_relativo = proporcoes_atraso / proporcao_total_atraso

print("Proporções de atraso por quartil:")
print(proporcoes_atraso)
print("\nRisco relativo por quartil em relação à proporção total de atrasos:")
print(risco_relativo)

import pandas as pd
from scipy.stats import spearmanr
# Calcular a correlação de Spearman
correlation, p_value = spearmanr(df['ARR_DELAY'], df['DISTANCE'])
print(f'Correlação de Spearman - Distância e Tempo de Voo: {correlation}')
print(f'Valor-p: {p_value}')

"""## Hipótese 4 | Teste Z-Score"""

import pandas as pd
from scipy import stats
# Adicionar uma coluna para indicar se o voo é na madrugada, usando CAT_CRS_DEP_TIME
df['IS_MIDNIGHT'] = df['CAT_CRS_DEP_TIME'].apply(lambda x: 1 if x == 'Late Night' else 0)
# Calcular o número de voos e o número de voos com atraso para cada grupo
midnight_flights = df[df['IS_MIDNIGHT'] == 1]
non_midnight_flights = df[df['IS_MIDNIGHT'] == 0]
# Contar voos e voos com atraso
n_midnight = len(midnight_flights)
n_non_midnight = len(non_midnight_flights)
delayed_midnight = midnight_flights['HAS_DELAY'].sum()
delayed_non_midnight = non_midnight_flights['HAS_DELAY'].sum()
# Calcular proporções de atraso
p_midnight = delayed_midnight / n_midnight
p_non_midnight = delayed_non_midnight / n_non_midnight
# Total de voos e voos com atraso
total_flights = n_midnight + n_non_midnight
total_delayed = delayed_midnight + delayed_non_midnight
# Proporção combinada de atraso
p_combined = total_delayed / total_flights
# Calcular o teste Z de duas proporções
se = ((p_combined * (1 - p_combined)) * (1 / n_midnight + 1 / n_non_midnight)) ** 0.5
z = (p_midnight - p_non_midnight) / se
# Calcular o valor-p para o teste
p_value = 2 * (1 - stats.norm.cdf(abs(z)))
# Exibir resultados
print(f'Proporção de atrasos na madrugada: {p_midnight:.2f}')
print(f'Proporção de atrasos fora da madrugada: {p_non_midnight:.2f}')
print(f'Z-Score: {z:.2f}')
print(f'Valor-p: {p_value:.4f}')
if p_value < 0.05:
    print("Há uma diferença significativa entre as proporções de atraso na madrugada e fora da madrugada.")
else:
    print("Não há uma diferença significativa entre as proporções de atraso na madrugada e fora da madrugada.")

"""## Criando tabela auxiliar "ORIGIN_CITY"
"""

# Agrupar por 'ORIGIN_CITY' e calcular os somatórios e contagens
tabela_origin_city = df.groupby('ORIGIN_CITY').agg({
    'FL_NUMBER': 'count',  # Contagem de 'FL_NUMBER'
    'DEP_DELAY': 'sum',  # Somatório de 'DEP_DELAY'
    'ARR_DELAY': 'sum',  # Somatório de 'ARR_DELAY'
    'DELAY_DUE_CARRIER': 'sum',  # Somatório de 'DELAY_DUE_CARRIER'
    'DELAY_DUE_WEATHER': 'sum',  # Somatório de 'DELAY_DUE_WEATHER'
    'DELAY_DUE_NAS': 'sum',  # Somatório de 'DELAY_DUE_NAS'
    'DELAY_DUE_SECURITY': 'sum',  # Somatório de 'DELAY_DUE_SECURITY'
    'DELAY_DUE_LATE_AIRCRAFT': 'sum',  # Somatório de 'DELAY_DUE_LATE_AIRCRAFT'
    'DISTANCE': 'sum',  # Somatório de 'DISTANCE'
    'AIR_TIME': 'sum',  # Somatório de 'AIR_TIME'
    'HAS_DELAY': 'sum'  # Somatório de 'DELAY_FLAG'
}).reset_index()
# Renomear as colunas para maior clareza
tabela_origin_city = tabela_origin_city.rename(columns={
    'FL_NUMBER': 'TOTAL_FLIGHTS',
    'DEP_DELAY': 'TOTAL_DEP_DELAY',
    'ARR_DELAY': 'TOTAL_ARR_DELAY',
    'DELAY_DUE_CARRIER': 'TOTAL_DELAY_DUE_CARRIER',
    'DELAY_DUE_WEATHER': 'TOTAL_DELAY_DUE_WEATHER',
    'DELAY_DUE_NAS': 'TOTAL_DELAY_DUE_NAS',
    'DELAY_DUE_SECURITY': 'TOTAL_DELAY_DUE_SECURITY',
    'DELAY_DUE_LATE_AIRCRAFT': 'TOTAL_DELAY_DUE_LATE_AIRCRAFT',
    'DISTANCE': 'TOTAL_DISTANCE',
    'AIR_TIME': 'TOTAL_AIR_TIME',
    'HAS_DELAY': 'TOTAL_DELAY_FLAG'
})
# Verificar a tabela auxiliar
print(tabela_origin_city.head())

tabela_origin_city

"""## Correlação da tabela auxiliar  | ORIGIN_CITY"""

correlation_matrix = tabela_origin_city[['TOTAL_FLIGHTS', 'TOTAL_DEP_DELAY', 'TOTAL_ARR_DELAY', 'TOTAL_DELAY_DUE_CARRIER', 'TOTAL_DELAY_DUE_WEATHER', 'TOTAL_DELAY_DUE_NAS', 'TOTAL_DELAY_DUE_SECURITY', 'TOTAL_DELAY_DUE_LATE_AIRCRAFT', 'TOTAL_DISTANCE', 'TOTAL_AIR_TIME', 'TOTAL_DELAY_FLAG']].corr()
corr = correlation_matrix.round(2)
mask = np.triu(np.ones_like(corr, dtype=bool))
f, ax = plt.subplots(figsize=(11, 9))
cmap = sns.diverging_palette(230, 20, as_cmap=True)
sns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0, square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot=True, fmt=".2f")
plt.show()

"""## Taxa de atrasos por ORIGIN"""

# Calcular a taxa de atrasos por origin
tabela_origin_city['delay_rate'] = (tabela_origin_city['TOTAL_DELAY_FLAG'] / tabela_origin_city['TOTAL_FLIGHTS']) * 100

# Ordenar a tabela por 'delay_rate' em ordem decrescente
tabela_origin_city_sorted = tabela_origin_city.sort_values(by=['delay_rate'], ascending=False)

# Exibindo a taxa de atrasos por origin ordenada
print("Taxa de atrasos por operadora % (decrescente):")
print(tabela_origin_city_sorted[['ORIGIN_CITY', 'delay_rate']])

"""## Criando tabela auxiliar | AIRLINE_NAME"""

# Agrupar por 'AIRLINE_NAME' e calcular os somatórios e contagens
tabela_airline_name = df.groupby('AIRLINE_NAME').agg({
    'FL_NUMBER': 'count',  # Contagem de 'FL_NUMBER'
    'DEP_DELAY': 'sum',  # Somatório de 'DEP_DELAY'
    'ARR_DELAY': 'sum',  # Somatório de 'ARR_DELAY'
    'DELAY_DUE_CARRIER': 'sum',  # Somatório de 'DELAY_DUE_CARRIER'
    'DELAY_DUE_WEATHER': 'sum',  # Somatório de 'DELAY_DUE_WEATHER'
    'DELAY_DUE_NAS': 'sum',  # Somatório de 'DELAY_DUE_NAS'
    'DELAY_DUE_SECURITY': 'sum',  # Somatório de 'DELAY_DUE_SECURITY'
    'DELAY_DUE_LATE_AIRCRAFT': 'sum',  # Somatório de 'DELAY_DUE_LATE_AIRCRAFT'
    'DISTANCE': 'sum',  # Somatório de 'DISTANCE'
    'AIR_TIME': 'sum',  # Somatório de 'AIR_TIME'
    'HAS_DELAY': 'sum'  # Somatório de 'DELAY_FLAG'
}).reset_index()
# Renomear as colunas para maior clareza
tabela_airline_name = tabela_airline_name.rename(columns={
    'FL_NUMBER': 'TOTAL_FLIGHTS',
    'DEP_DELAY': 'TOTAL_DEP_DELAY',
    'ARR_DELAY': 'TOTAL_ARR_DELAY',
    'DELAY_DUE_CARRIER': 'TOTAL_DELAY_DUE_CARRIER',
    'DELAY_DUE_WEATHER': 'TOTAL_DELAY_DUE_WEATHER',
    'DELAY_DUE_NAS': 'TOTAL_DELAY_DUE_NAS',
    'DELAY_DUE_SECURITY': 'TOTAL_DELAY_DUE_SECURITY',
    'DELAY_DUE_LATE_AIRCRAFT': 'TOTAL_DELAY_DUE_LATE_AIRCRAFT',
    'DISTANCE': 'TOTAL_DISTANCE',
    'AIR_TIME': 'TOTAL_AIR_TIME',
    'HAS_DELAY': 'TOTAL_DELAY_FLAG'
})
# Verificar a tabela auxiliar
print(tabela_airline_name.head())

tabela_airline_name

"""## Correlação da tabela auxiliar | AIRLINE_NAME"""

correlation_matrix = tabela_airline_name[['TOTAL_FLIGHTS', 'TOTAL_DEP_DELAY', 'TOTAL_ARR_DELAY', 'TOTAL_DELAY_DUE_CARRIER', 'TOTAL_DELAY_DUE_WEATHER', 'TOTAL_DELAY_DUE_NAS', 'TOTAL_DELAY_DUE_SECURITY', 'TOTAL_DELAY_DUE_LATE_AIRCRAFT', 'TOTAL_DISTANCE', 'TOTAL_AIR_TIME', 'TOTAL_DELAY_FLAG']].corr()
corr = correlation_matrix.round(2)
mask = np.triu(np.ones_like(corr, dtype=bool))
f, ax = plt.subplots(figsize=(11, 9))
cmap = sns.diverging_palette(230, 20, as_cmap=True)
sns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0, square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot=True, fmt=".2f")
plt.show()

"""## Verifcando a normalidade dos dados"""

# Lista de colunas a serem testadas
colunas = [
    'TOTAL_FLIGHTS', 'TOTAL_DEP_DELAY', 'TOTAL_ARR_DELAY',
    'TOTAL_DELAY_DUE_CARRIER', 'TOTAL_DELAY_DUE_WEATHER',
    'TOTAL_DELAY_DUE_NAS', 'TOTAL_DELAY_DUE_SECURITY',
    'TOTAL_DELAY_DUE_LATE_AIRCRAFT', 'TOTAL_DISTANCE',
    'TOTAL_AIR_TIME', 'TOTAL_DELAY_FLAG'
]

for coluna in colunas:
    if coluna not in tabela_airline_name.columns:
        print(f"A coluna {coluna} não está no DataFrame.")
        continue

    # Remover valores ausentes
    df_coluna = tabela_airline_name[coluna].dropna()

    # Verificar o tamanho da amostra e limitar a 5000 observações
    if len(df_coluna) > 5000:
        df_coluna = df_coluna.sample(5000, random_state=42)

    # Aplicar o teste de Shapiro-Wilk
    stat, p_valor = shapiro(df_coluna)
    print(f"\nTeste de Shapiro-Wilk para {coluna}:")
    print("Estatística de teste:", stat)
    print("p-valor:", p_valor)

    # Interpretação
    if p_valor > 0.05:
        print(f"Os dados da coluna {coluna} seguem uma distribuição normal (não rejeitamos H0).")
    else:
        print(f"Os dados da coluna {coluna} não seguem uma distribuição normal (rejeitamos H0).")

"""## Taxa de atrasos por AIRLINE_NAME"""

delay_stats = df.groupby('AIRLINE_NAME').agg(
    total_delay=('HAS_DELAY', 'sum'),
    num_flights=('FL_NUMBER', 'count')
).reset_index()

# Calcular a média de atraso por voo
delay_stats['avg_delay_per_flight'] = delay_stats['total_delay'] / delay_stats['num_flights'] *100

# Ordenar as companhias aéreas pela média de atraso por voo em ordem decrescente
delay_stats = delay_stats.sort_values(by='avg_delay_per_flight', ascending=False)

# Exibir as estatísticas
print(delay_stats)

# Calcular os quartis para TOTAL_FLIGHTS
quartis_TOTAL_FLIGHTS = tabela_airline_name['TOTAL_FLIGHTS'].quantile([0.25, 0.75])
print("Quartis para TOTAL_FLIGHTS:")
print(quartis_TOTAL_FLIGHTS)

# Definir os grupos com base nos quartis calculados
tabela_airline_name['flight_group'] = pd.cut(tabela_airline_name['TOTAL_FLIGHTS'],
                                             bins=[-float('inf'), quartis_TOTAL_FLIGHTS[0.75], float('inf')],
                                             labels=['Baixo', 'Alto'])

print("DataFrame com grupos definidos:")
print(tabela_airline_name)

# Contar a quantidade de companhias aéreas em cada grupo
group_counts = tabela_airline_name['flight_group'].value_counts()

# Plotar o gráfico de barras
plt.figure(figsize=(8, 5))
bars = plt.bar(group_counts.index, group_counts.values, color='skyblue')
plt.xlabel('Grupo de Voos')
plt.ylabel('Número de Companhias Aéreas')
plt.title('Distribuição de Companhias Aéreas por Grupo de Voos')

# Adicionar os valores acima das barras
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.2, int(yval), ha='center', va='bottom', fontsize=10)

plt.tight_layout()
plt.show()

"""##  Hipótese 3 | Teste de Mannwhitneyu U | Taxa de atrasos"""

from scipy.stats import mannwhitneyu

# Selecionar os grupos
grupo_baixo = tabela_airline_name[tabela_airline_name['flight_group'] == 'Baixo']['TOTAL_DELAY_FLAG']
grupo_alto = tabela_airline_name[tabela_airline_name['flight_group'] == 'Alto']['TOTAL_DELAY_FLAG']

# Teste de Mann-Whitney
stat, p_value = mannwhitneyu(grupo_baixo, grupo_alto)
print(f'Estatística do Teste de Mann-Whitney: {stat}, Valor-p: {p_value}')

# Interpretação do resultado
if p_value < 0.05:
    print("Há uma diferença significativa nas médias de atraso entre os grupos 'Baixo' e 'Alto'.")
else:
    print("Não há diferença significativa nas médias de atraso entre os grupos 'Baixo' e 'Alto'.")

"""## Criando tabela auxiliar "DEST_CITY"



"""

# Agrupar por 'DEST_CITY' e calcular os somatórios e contagens
tabela_dest_city = df.groupby('DEST_CITY').agg({
    'FL_NUMBER': 'count',  # Contagem de 'FL_NUMBER'
    'DEP_DELAY': 'sum',  # Somatório de 'DEP_DELAY'
    'ARR_DELAY': 'sum',  # Somatório de 'ARR_DELAY'
    'DELAY_DUE_CARRIER': 'sum',  # Somatório de 'DELAY_DUE_CARRIER'
    'DELAY_DUE_WEATHER': 'sum',  # Somatório de 'DELAY_DUE_WEATHER'
    'DELAY_DUE_NAS': 'sum',  # Somatório de 'DELAY_DUE_NAS'
    'DELAY_DUE_SECURITY': 'sum',  # Somatório de 'DELAY_DUE_SECURITY'
    'DELAY_DUE_LATE_AIRCRAFT': 'sum',  # Somatório de 'DELAY_DUE_LATE_AIRCRAFT'
    'DISTANCE': 'sum',  # Somatório de 'DISTANCE'
    'AIR_TIME': 'sum',  # Somatório de 'AIR_TIME'
    'DELAY_FLAG': 'sum'  # Somatório de 'DELAY_FLAG'
}).reset_index()
# Renomear as colunas para maior clareza
tabela_dest_city = tabela_dest_city.rename(columns={
    'FL_NUMBER': 'TOTAL_FLIGHTS',
    'DEP_DELAY': 'TOTAL_DEP_DELAY',
    'ARR_DELAY': 'TOTAL_ARR_DELAY',
    'DELAY_DUE_CARRIER': 'TOTAL_DELAY_DUE_CARRIER',
    'DELAY_DUE_WEATHER': 'TOTAL_DELAY_DUE_WEATHER',
    'DELAY_DUE_NAS': 'TOTAL_DELAY_DUE_NAS',
    'DELAY_DUE_SECURITY': 'TOTAL_DELAY_DUE_SECURITY',
    'DELAY_DUE_LATE_AIRCRAFT': 'TOTAL_DELAY_DUE_LATE_AIRCRAFT',
    'DISTANCE': 'TOTAL_DISTANCE',
    'AIR_TIME': 'TOTAL_AIR_TIME',
    'DELAY_FLAG': 'TOTAL_DELAY_FLAG'
})
# Verificar a tabela auxiliar
print(tabela_dest_city.head())

tabela_dest_city

"""## Teste shapiro"""

# Lista de colunas a serem testadas
colunas = [
    'TOTAL_FLIGHTS', 'TOTAL_DEP_DELAY', 'TOTAL_ARR_DELAY',
    'TOTAL_DELAY_DUE_CARRIER', 'TOTAL_DELAY_DUE_WEATHER',
    'TOTAL_DELAY_DUE_NAS', 'TOTAL_DELAY_DUE_SECURITY',
    'TOTAL_DELAY_DUE_LATE_AIRCRAFT', 'TOTAL_DISTANCE',
    'TOTAL_AIR_TIME', 'TOTAL_DELAY_FLAG'
]

for coluna in colunas:
    if coluna not in tabela_dest_city.columns:
        print(f"A coluna {coluna} não está no DataFrame.")
        continue

    # Remover valores ausentes
    df_coluna = tabela_dest_city[coluna].dropna()

    # Verificar o tamanho da amostra e limitar a 5000 observações
    if len(df_coluna) > 5000:
        df_coluna = df_coluna.sample(5000, random_state=42)

    # Aplicar o teste de Shapiro-Wilk
    stat, p_valor = shapiro(df_coluna)
    print(f"\nTeste de Shapiro-Wilk para {coluna}:")
    print("Estatística de teste:", stat)
    print("p-valor:", p_valor)

    # Interpretação
    if p_valor > 0.05:
        print(f"Os dados da coluna {coluna} seguem uma distribuição normal (não rejeitamos H0).")
    else:
        print(f"Os dados da coluna {coluna} não seguem uma distribuição normal (rejeitamos H0).")

"""## Correlação da tabela auxiliar | DEST_CITY

"""

correlation_matrix = tabela_dest_city[['TOTAL_FLIGHTS', 'TOTAL_DEP_DELAY', 'TOTAL_ARR_DELAY', 'TOTAL_DELAY_DUE_CARRIER', 'TOTAL_DELAY_DUE_WEATHER', 'TOTAL_DELAY_DUE_NAS', 'TOTAL_DELAY_DUE_SECURITY', 'TOTAL_DELAY_DUE_LATE_AIRCRAFT', 'TOTAL_DISTANCE', 'TOTAL_AIR_TIME', 'TOTAL_DELAY_FLAG']].corr()
corr = correlation_matrix.round(2)
mask = np.triu(np.ones_like(corr, dtype=bool))
f, ax = plt.subplots(figsize=(11, 9))
cmap = sns.diverging_palette(230, 20, as_cmap=True)
sns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0, square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot=True, fmt=".2f")
plt.show()

"""## Taxa de atrasos por DEST_CITY"""

# Calcular a taxa de atrasos por destino
tabela_dest_city['delay_rate'] = (tabela_dest_city['TOTAL_DELAY_FLAG'] / tabela_dest_city['TOTAL_FLIGHTS']) * 100

# Ordenar a tabela por 'delay_rate' em ordem decrescente
tabela_dest_city_sorted = tabela_dest_city.sort_values(by=['delay_rate'], ascending=False)

# Exibindo a taxa de atrasos por destino ordenada
print("Taxa de atrasos por operadora % (decrescente):")
print(tabela_dest_city_sorted[['DEST_CITY', 'delay_rate']])

"""## Criando tabela auxiliar "FLIGHT_ROUTE"
"""

# Agrupar por 'ORIGIN_CITY' e calcular os somatórios e contagens
tabela_flight_route = df.groupby('FLIGHT_ROUTE').agg({
    'FL_NUMBER': 'count',  # Contagem de 'FL_NUMBER'
    'DEP_DELAY': 'sum',  # Somatório de 'DEP_DELAY'
    'ARR_DELAY': 'sum',  # Somatório de 'ARR_DELAY'
    'DELAY_DUE_CARRIER': 'sum',  # Somatório de 'DELAY_DUE_CARRIER'
    'DELAY_DUE_WEATHER': 'sum',  # Somatório de 'DELAY_DUE_WEATHER'
    'DELAY_DUE_NAS': 'sum',  # Somatório de 'DELAY_DUE_NAS'
    'DELAY_DUE_SECURITY': 'sum',  # Somatório de 'DELAY_DUE_SECURITY'
    'DELAY_DUE_LATE_AIRCRAFT': 'sum',  # Somatório de 'DELAY_DUE_LATE_AIRCRAFT'
    'DISTANCE': 'sum',  # Somatório de 'DISTANCE'
    'AIR_TIME': 'sum',  # Somatório de 'AIR_TIME'
    'HAS_DELAY': 'sum'  # Somatório de 'DELAY_FLAG'
}).reset_index()
# Renomear as colunas para maior clareza
tabela_flight_route = tabela_flight_route.rename(columns={
    'FL_NUMBER': 'TOTAL_FLIGHTS',
    'DEP_DELAY': 'TOTAL_DEP_DELAY',
    'ARR_DELAY': 'TOTAL_ARR_DELAY',
    'DELAY_DUE_CARRIER': 'TOTAL_DELAY_DUE_CARRIER',
    'DELAY_DUE_WEATHER': 'TOTAL_DELAY_DUE_WEATHER',
    'DELAY_DUE_NAS': 'TOTAL_DELAY_DUE_NAS',
    'DELAY_DUE_SECURITY': 'TOTAL_DELAY_DUE_SECURITY',
    'DELAY_DUE_LATE_AIRCRAFT': 'TOTAL_DELAY_DUE_LATE_AIRCRAFT',
    'DISTANCE': 'TOTAL_DISTANCE',
    'AIR_TIME': 'TOTAL_AIR_TIME',
    'HAS_DELAY': 'TOTAL_DELAY_FLAG'
})
# Verificar a tabela auxiliar
print(tabela_flight_route.head())

tabela_flight_route

"""## Taxa de atrasos por rota"""

tabela_flight_route['delay_rate'] = (tabela_flight_route['TOTAL_DELAY_FLAG'] / tabela_flight_route['TOTAL_FLIGHTS']) * 100
tabela_flight_route_sorted = tabela_flight_route.sort_values(by=['delay_rate'], ascending=False)

print("Taxa de atrasos por rotas % (decrescente):")
print(tabela_flight_route_sorted[['FLIGHT_ROUTE', 'delay_rate']])

"""## Correlação tabela auxiliar | FLIGHT_ROUTE"""

correlation_matrix = tabela_flight_route[['TOTAL_FLIGHTS', 'TOTAL_DEP_DELAY', 'TOTAL_ARR_DELAY', 'TOTAL_DELAY_DUE_CARRIER', 'TOTAL_DELAY_DUE_WEATHER', 'TOTAL_DELAY_DUE_NAS', 'TOTAL_DELAY_DUE_SECURITY', 'TOTAL_DELAY_DUE_LATE_AIRCRAFT', 'TOTAL_DISTANCE', 'TOTAL_AIR_TIME', 'TOTAL_DELAY_FLAG']].corr()
corr = correlation_matrix.round(2)
mask = np.triu(np.ones_like(corr, dtype=bool))
f, ax = plt.subplots(figsize=(11, 9))
cmap = sns.diverging_palette(230, 20, as_cmap=True)
sns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0, square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot=True, fmt=".2f")
plt.show()

"""## Média de atrasos por voo"""

delay_stats = df.groupby('FLIGHT_ROUTE').agg(
    total_delay=('DEP_DELAY', 'sum'),
    num_flights=('FL_NUMBER', 'count')
).reset_index()

# Calcular a média de atraso por voo
delay_stats['avg_delay_per_flight'] = delay_stats['total_delay'] / delay_stats['num_flights'] *100

# Ordenar as companhias aéreas pela média de atraso por voo em ordem decrescente
delay_stats = delay_stats.sort_values(by='avg_delay_per_flight', ascending=False)

# Exibir as estatísticas
print(delay_stats)

"""## Regressão linear"""

import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from scipy.stats import pearsonr

# Plote o Gráfico de Dispersão e Calcule o índice de correlação
X = df['DELAY_DUE_CARRIER'].values
Y = df['DEP_DELAY'].values

# Calcule o índice de correlação
r = pearsonr(X, Y)
print(f'Coeficiente de correlação: {r}')

# Separe dados de treino e teste
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# Redimensione os dados para fazer a regressão linear
x_train = x_train.reshape(-1, 1)
x_test = x_test.reshape(-1, 1)

# Treine o modelo
reg = LinearRegression()
reg.fit(x_train, y_train)
pred = reg.predict(x_test)

# Ajuste o tamanho da figura
plt.figure(figsize=(8, 6))
# Plotando o Gráfico de Dispersão
plt.scatter(X, Y, color="blue")
plt.plot(x_test, pred, color="pink")
plt.title("Atraso pelo Companhia Aérea vs Tempo de Delay na Partida")
plt.xlabel("Atraso pelo Companhia Aérea")
plt.ylabel("Tempo de Delay na Partida")
plt.show()

# Verifique a representação do modelo
r_squared = r2_score(y_test, pred)
print(f'Coeficiente r2: {r_squared}')

# Calcule os resíduos
residual = y_test - pred

# Ajuste o tamanho da figura
plt.figure(figsize=(8, 6))
# Plotando o histograma dos resíduos
plt.hist(residual, rwidth=0.9)
plt.title('Resíduos')
plt.xlabel('Resíduos (DEP_DELAY)')
plt.ylabel('Frequência Absoluta')
plt.show()

# Plotando o Gráfico de Dispersão e Calculando o índice de correlação
X = df['DELAY_DUE_WEATHER'].values
Y = df['DEP_DELAY'].values

# Calculando o índice de correlação
r = pearsonr(X, Y)
print(f'Coeficiente de correlação: {r}')

# Separarando os dados de treino e teste
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# Redimensionando os dados para fazer a regressão linear
x_train = x_train.reshape(-1, 1)
x_test = x_test.reshape(-1, 1)

# Treinando o modelo
reg = LinearRegression()
reg.fit(x_train, y_train)
pred = reg.predict(x_test)

# Ajuste o tamanho da figura
plt.figure(figsize=(8, 6))  # Defina a largura e altura desejadas
# Plotando o Gráfico de Dispersão
plt.scatter(X, Y, color="blue")
plt.plot(x_test, pred, color="pink")
plt.title("Atraso pelo Companhia Aérea vs Tempo de Delay na Partida")
plt.xlabel("Atraso pelo Companhia Aérea")
plt.ylabel("Tempo de Delay na Partida")
plt.show()

# Verificando a representação do modelo
r_squared = r2_score(y_test, pred)
print(f'Coeficiente r2: {r_squared}')

# Calculando os resíduos
residual = y_test - pred

# Ajustando o tamanho da figura
plt.figure(figsize=(8, 6))
# Plotando o histograma dos resíduos
plt.hist(residual, rwidth=0.9)
plt.title('Resíduos')
plt.xlabel('Resíduos (DEP_DELAY)')
plt.ylabel('Frequência Absoluta')
plt.show()

# Plotando o Gráfico de Dispersão e Calculando o índice de correlação
X = df['DELAY_DUE_NAS'].values
Y = df['DEP_DELAY'].values

# Calculando o índice de correlação
r = pearsonr(X, Y)
print(f'Coeficiente de correlação: {r}')

# Separarando os dados de treino e teste
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# Redimensionando os dados para fazer a regressão linear
x_train = x_train.reshape(-1, 1)
x_test = x_test.reshape(-1, 1)

# Treinando o modelo
reg = LinearRegression()
reg.fit(x_train, y_train)
pred = reg.predict(x_test)

# Ajuste o tamanho da figura
plt.figure(figsize=(8, 6))  # Defina a largura e altura desejadas
# Plotando o Gráfico de Dispersão
plt.scatter(X, Y, color="blue")
plt.plot(x_test, pred, color="pink")
plt.title("Atraso pelo Companhia Aérea vs Tempo de Delay na Partida")
plt.xlabel("Atraso pelo Companhia Aérea")
plt.ylabel("Tempo de Delay na Partida")
plt.show()

# Verificando a representação do modelo
r_squared = r2_score(y_test, pred)
print(f'Coeficiente r2: {r_squared}')

# Calculando os resíduos
residual = y_test - pred

# Ajustando o tamanho da figura
plt.figure(figsize=(8, 6))
# Plotando o histograma dos resíduos
plt.hist(residual, rwidth=0.9)
plt.title('Resíduos')
plt.xlabel('Resíduos (DEP_DELAY)')
plt.ylabel('Frequência Absoluta')
plt.show()

# Plotando o Gráfico de Dispersão e Calculando o índice de correlação
X = df['ARR_DELAY'].values
Y = df['DEP_DELAY'].values

# Calculando o índice de correlação
r, _ = pearsonr(X, Y)
print(f'Coeficiente de correlação: {r}')

# Separarando os dados de treino e teste
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# Redimensionando os dados para fazer a regressão linear
x_train = x_train.reshape(-1, 1)
x_test = x_test.reshape(-1, 1)

# Treinando o modelo
reg = LinearRegression()
reg.fit(x_train, y_train)
pred = reg.predict(x_test)

# Ajuste o tamanho da figura
fig, ax = plt.subplots(figsize=(8, 6))
ax.set_facecolor("#fbe3e3")
# Plotando o Gráfico de Dispersão
ax.scatter(X, Y, color="#064d84")
ax.plot(x_test, pred, color="#c96869")
# Ajustando os títulos e as labels
ax.set_title("Hórario previsto de chegada e o real vs tempo de delay na partida", color='#064d84')
ax.set_xlabel("Tempo de Delay na Chegada", color='#064d84')
ax.set_ylabel("Tempo de Delay na Partida", color='#064d84')

# Ajustando a cor dos eixos
ax.tick_params(axis='x', colors='#064d84')
ax.tick_params(axis='y', colors='#064d84')
ax.spines['bottom'].set_color('#064d84')
ax.spines['left'].set_color('#064d84')

plt.show()

# Verificando a representação do modelo
r_squared = r2_score(y_test, pred)
print(f'Coeficiente r2: {r_squared}')

# Calculando os resíduos
residual = y_test - pred

# Ajustando o tamanho da figura
fig, ax = plt.subplots(figsize=(8, 6))
ax.set_facecolor("#fbe3e3")
# Plotando o histograma dos resíduos
ax.hist(residual, rwidth=0.9)
# Ajustando os títulos e as labels
ax.set_title('Resíduos', color='#064d84')
ax.set_xlabel('Resíduos (DEP_DELAY)', color='#064d84')
ax.set_ylabel('Frequência Absoluta', color='#064d84')

# Ajustando a cor dos eixos
ax.tick_params(axis='x', colors='#064d84')
ax.tick_params(axis='y', colors='#064d84')
ax.spines['bottom'].set_color('#064d84')
ax.spines['left'].set_color('#064d84')

# Ajustando a borda da caixa de texto para rosa
for spine in ax.spines.values():
    spine.set_edgecolor("#c96869")

plt.show()

# Plotando o Gráfico de Dispersão e Calculando o índice de correlação
X = df['DELAY_DUE_SECURITY'].values
Y = df['DEP_DELAY'].values

# Calculando o índice de correlação
r = pearsonr(X, Y)
print(f'Coeficiente de correlação: {r}')

# Separarando os dados de treino e teste
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# Redimensionando os dados para fazer a regressão linear
x_train = x_train.reshape(-1, 1)
x_test = x_test.reshape(-1, 1)

# Treinando o modelo
reg = LinearRegression()
reg.fit(x_train, y_train)
pred = reg.predict(x_test)

# Ajuste o tamanho da figura
plt.figure(figsize=(8, 6))  # Defina a largura e altura desejadas
# Plotando o Gráfico de Dispersão
plt.scatter(X, Y, color="blue")
plt.plot(x_test, pred, color="pink")
plt.title("Atraso pelo Companhia Aérea vs Tempo de Delay na Partida")
plt.xlabel("Atraso pelo Companhia Aérea")
plt.ylabel("Tempo de Delay na Partida")
plt.show()

# Verificando a representação do modelo
r_squared = r2_score(y_test, pred)
print(f'Coeficiente r2: {r_squared}')

# Calculando os resíduos
residual = y_test - pred

# Ajustando o tamanho da figura
plt.figure(figsize=(8, 6))
# Plotando o histograma dos resíduos
plt.hist(residual, rwidth=0.9)
plt.title('Resíduos')
plt.xlabel('Resíduos (DEP_DELAY)')
plt.ylabel('Frequência Absoluta')
plt.show()

# Plotando o Gráfico de Dispersão e Calculando o índice de correlação
X = df['DELAY_DUE_LATE_AIRCRAFT'].values
Y = df['ARR_DELAY'].values

# Calculando o índice de correlação
r = pearsonr(X, Y)
print(f'Coeficiente de correlação: {r}')

# Separarando os dados de treino e teste
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# Redimensionando os dados para fazer a regressão linear
x_train = x_train.reshape(-1, 1)
x_test = x_test.reshape(-1, 1)

# Treinando o modelo
reg = LinearRegression()
reg.fit(x_train, y_train)
pred = reg.predict(x_test)

# Ajuste o tamanho da figura
plt.figure(figsize=(8, 6))  # Defina a largura e altura desejadas
# Plotando o Gráfico de Dispersão
plt.scatter(X, Y, color="blue")
plt.plot(x_test, pred, color="pink")
plt.title("Atraso pelo Companhia Aérea vs Tempo de Delay na Partida")
plt.xlabel("Atraso pelo Companhia Aérea")
plt.ylabel("Tempo de Delay na Partida")
plt.show()

# Verificando a representação do modelo
r_squared = r2_score(y_test, pred)
print(f'Coeficiente r2: {r_squared}')

# Calculando os resíduos
residual = y_test - pred

# Ajustando o tamanho da figura
plt.figure(figsize=(8, 6))
# Plotando o histograma dos resíduos
plt.hist(residual, rwidth=0.9)
plt.title('Resíduos')
plt.xlabel('Resíduos (DEP_DELAY)')
plt.ylabel('Frequência Absoluta')
plt.show()

# Plotando o Gráfico de Dispersão e Calculando o índice de correlação
X = df['DISTANCE'].values
Y = df['DEP_DELAY'].values

# Calculando o índice de correlação
r = pearsonr(X, Y)
print(f'Coeficiente de correlação: {r}')

# Separarando os dados de treino e teste
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# Redimensionando os dados para fazer a regressão linear
x_train = x_train.reshape(-1, 1)
x_test = x_test.reshape(-1, 1)

# Treinando o modelo
reg = LinearRegression()
reg.fit(x_train, y_train)
pred = reg.predict(x_test)

# Ajuste o tamanho da figura
plt.figure(figsize=(8, 6))  # Defina a largura e altura desejadas
# Plotando o Gráfico de Dispersão
plt.scatter(X, Y, color="blue")
plt.plot(x_test, pred, color="pink")
plt.title("Atraso pelo Companhia Aérea vs Tempo de Delay na Partida")
plt.xlabel("Atraso pelo Companhia Aérea")
plt.ylabel("Tempo de Delay na Partida")
plt.show()

# Verificando a representação do modelo
r_squared = r2_score(y_test, pred)
print(f'Coeficiente r2: {r_squared}')

# Calculando os resíduos
residual = y_test - pred

# Ajustando o tamanho da figura
plt.figure(figsize=(8, 6))
# Plotando o histograma dos resíduos
plt.hist(residual, rwidth=0.9)
plt.title('Resíduos')
plt.xlabel('Resíduos (DEP_DELAY)')
plt.ylabel('Frequência Absoluta')
plt.show()

"""## Regressão logística"""

df = df[colunas_selecionadas].copy()
# Criar a variável binária para DEP_DELAY
df['HAS_DELAY'] = np.where(df['DEP_DELAY'] >= 10, 1, 0)
# Criar variáveis binárias para DAY_OF_WEEK
df['DAY_OF_WEEK_BINARY'] = np.where(df['DAY_OF_WEEK'].isin(['Monday', 'Wednesday']), 1, 0)
# Criar variáveis binárias para CAT_CRS_DEP_TIME
df['CAT_CRS_DEP_TIME_BINARY'] = np.where(df['CAT_CRS_DEP_TIME'].isin(['Afternoon', 'Evening']), 1, 0)
# Criar variáveis binárias para tipos de atraso
df['DELAY_DUE_CARRIER_BINARY'] = np.where(df['DELAY_DUE_CARRIER'] > 10, 1, 0)
df['DELAY_DUE_NAS_BINARY'] = np.where(df['DELAY_DUE_NAS'] > 10, 1, 0)
df['DELAY_DUE_WEATHER_BINARY'] = np.where(df['DELAY_DUE_WEATHER'] > 10, 1, 0)
df['DELAY_DUE_SECURITY_BINARY'] = np.where(df['DELAY_DUE_SECURITY'] > 10, 1, 0)
df['DELAY_DUE_LATE_AIRCRAFT_BINARY'] = np.where(df['DELAY_DUE_LATE_AIRCRAFT'] > 10, 1, 0)
# Calcular a taxa de atraso por rotas
tabela_flight_route['delay_rate'] = (tabela_flight_route['TOTAL_DELAY_FLAG'] / tabela_flight_route['TOTAL_FLIGHTS']) * 100
# Criar uma coluna binária para FLIGHT_ROUTE com base na taxa de atraso
tabela_flight_route['FLIGHT_ROUTE_BINARY'] = np.where(tabela_flight_route['delay_rate'] >= 50, 1, 0)
# Mapear os valores binários de FLIGHT_ROUTE para o DataFrame original
flight_route_map = tabela_flight_route.set_index('FLIGHT_ROUTE')['FLIGHT_ROUTE_BINARY'].to_dict()
df['FLIGHT_ROUTE_BINARY'] = df['FLIGHT_ROUTE'].map(flight_route_map)
# Verificar se há valores nulos e substituí-los por 0 (caso alguma rota não esteja no map)
df['FLIGHT_ROUTE_BINARY'].fillna(0, inplace=True)
# Selecionar as colunas desejadas para a regressão logística
columns_for_logistic_regression = [
    'HAS_DELAY',
    'DAY_OF_WEEK_BINARY',
    'CAT_CRS_DEP_TIME_BINARY',
    'DELAY_DUE_CARRIER_BINARY',
    'DELAY_DUE_NAS_BINARY',
    'DELAY_DUE_WEATHER_BINARY',
    'DELAY_DUE_SECURITY_BINARY',
    'DELAY_DUE_LATE_AIRCRAFT_BINARY',
    'FLIGHT_ROUTE_BINARY'
]
df_logistic_regression = df[columns_for_logistic_regression]
# Verificar a nova tabela
print(df_logistic_regression.head())

df_logistic_regression

sns.countplot(x = df_logistic_regression['HAS_DELAY'])

df_logistic_regression['HAS_DELAY'].value_counts()

sem_atraso_percentage = df_logistic_regression[df_logistic_regression['HAS_DELAY'] == 0].shape[0] / df_logistic_regression.shape[0] * 100
com_atraso_percentage = df_logistic_regression[df_logistic_regression['HAS_DELAY'] == 1].shape[0] / df_logistic_regression.shape[0] * 100

print(f'The percentage of:\n'
      f'Sem_atraso: {sem_atraso_percentage:.2f}%\n'
      f'Com_atraso: {com_atraso_percentage:.2f}%\n'
      )

#dividindo os dados em conjuntos de treinamento e teste
x = df_logistic_regression.drop(['HAS_DELAY'], axis = 1) #features = variaveis independentes
y = df_logistic_regression['HAS_DELAY'] #target

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)
#treino: 70% e teste:30%

cols = df_logistic_regression.columns.drop(['HAS_DELAY'])
formula = 'HAS_DELAY ~ ' + ' + '.join(cols)
print(formula, '\n')

model = smf.glm(formula=formula, data=x_train.join(y_train), family=sm.families.Binomial())
logistic_fit = model.fit()
print(logistic_fit.summary())

# Fazer previsões nos dados de teste
y_pred_prob = logistic_fit.predict(x_test)
# Converter probabilidades em classes
y_pred = [1 if x > 0.5 else 0 for x in y_pred_prob]

from sklearn import metrics
from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns

# Calcular a matriz de confusão
cm = metrics.confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()

# Calcular as métricas de desempenho
accuracy = accuracy_score(y_test, y_pred)
acuracia_balanceada = balanced_accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Definindo as cores
cor_barra = "#fbe2e2"
cor_texto = "#064b85"

# Visualização gráfica da matriz
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap=sns.light_palette(cor_barra, as_cmap=True),
            annot_kws={"color": cor_texto}, xticklabels=['0 (Sem atraso)', '1 (Com atraso)'],
            yticklabels=['0 (Sem atraso)', '1 (Com atraso)'])
plt.xlabel('Predições', color=cor_texto)
plt.ylabel('Valores Reais', color=cor_texto)
plt.title('Matriz de Confusão', color=cor_texto)
plt.show()

print(f'True Positive: {tp}')
print(f'False Positive: {fp}')
print(f'False Negative: {fn}')
print(f'True Negative: {tn}')
print(f'Accuracy: {accuracy}')
print(f"Acurácia Balanceada: {acuracia_balanceada:.3f}")
print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1 Score: {f1}')

# Importar bibliotecas necessárias
import pandas as pd
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Definir as variáveis independentes
X = df_logistic_regression.drop(['HAS_DELAY'], axis=1)

# Calcular o VIF para cada variável independente
vif_data = pd.DataFrame()
vif_data["feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

print(vif_data)

"""## Regressão somente com 3 variaveis independentes"""

# Dividindo os dados em conjuntos de treinamento e teste
x = df_logistic_regression[['DAY_OF_WEEK_BINARY', 'CAT_CRS_DEP_TIME_BINARY', 'FLIGHT_ROUTE_BINARY']] # features = variaveis independentes
y = df_logistic_regression['HAS_DELAY'] # target
# treino: 70% e teste: 30%
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

# Criar o modelo de regressão logística
model = LogisticRegression()

# Treinar o modelo
model.fit(x_train, y_train)

# Prever os valores para o conjunto de teste
y_prob = model.predict_proba(x_test)[:, 1]
y_pred = model.predict(x_test)

# Calcular a curva ROC e a área sob a curva (AUC)
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = roc_auc_score(y_test, y_prob)

# Calcular a matriz de confusão
conf_matrix = confusion_matrix(y_test, y_pred)
true_positive = conf_matrix[1, 1]
false_positive = conf_matrix[0, 1]
false_negative = conf_matrix[1, 0]
true_negative = conf_matrix[0, 0]

# Calcular as métricas de desempenho
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_prob)
log_loss_value = log_loss(y_test, y_prob)

# Exibir as métricas de desempenho
print("Matriz de Confusão:")
print(conf_matrix)
print("True Positive:", true_positive)
print("False Positive:", false_positive)
print("False Negative:", false_negative)
print("True Negative:", true_negative)
print("Acurácia:", accuracy)
print("Precisão:", precision)
print("Recall:", recall)
print("F1-score:", f1)
print("AUC ROC:", roc_auc)
print("Log Loss:", log_loss_value)

# Definir as cores desejadas
cor_barra = "#fbe2e2"
cor_texto = "#064b85"

# Plotar a curva ROC
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color=cor_texto, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color=cor_barra, linestyle='--')
plt.xlabel('False Positive Rate', color=cor_texto)
plt.ylabel('True Positive Rate', color=cor_texto)
plt.title('Curva ROC', color=cor_texto)
plt.legend(loc="lower right")
plt.show()

# Criar uma nova paleta de cores
cmap = sns.color_palette([cor_barra, cor_texto])

# Visualização gráfica da matriz de confusão
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap=sns.light_palette(cor_barra, as_cmap=True),
            annot_kws={"color": cor_texto}, xticklabels=['0 (Sem atraso)', '1 (Com atraso)'],
            yticklabels=['0 (Sem atraso)', '1 (Com atraso)'])
plt.xlabel('Predições', color=cor_texto)
plt.ylabel('Valores Reais', color=cor_texto)
plt.title('Matriz de Confusão', color=cor_texto)
plt.show()

"""## Regressão logistica utilizando a técnica SMOTE devido ao desbalanceamento dos dados"""

# Divisão dos dados em conjuntos de treinamento e teste
x = df_logistic_regression.drop(['HAS_DELAY'], axis=1)  # Features
y = df_logistic_regression['HAS_DELAY']  # Target

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

# Aplicar SMOTE ao conjunto de treino
smote = SMOTE(random_state=42)
x_train_res, y_train_res = smote.fit_resample(x_train, y_train)

# Criação e ajuste do modelo de regressão logística
cols = df_logistic_regression.columns.drop(['HAS_DELAY'])
formula = 'HAS_DELAY ~ ' + ' + '.join(cols)
print(formula, '\n')

model = smf.glm(formula=formula, data=x_train_res.join(y_train_res), family=sm.families.Binomial())
logistic_fit = model.fit()
print(logistic_fit.summary())

# Fazer previsões nos dados de teste
y_pred_prob = logistic_fit.predict(x_test)
# Converter probabilidades em classes
y_pred = [1 if x > 0.5 else 0 for x in y_pred_prob]

# Calcular a matriz de confusão e métricas de desempenho
cm = metrics.confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()

accuracy = accuracy_score(y_test, y_pred)
acuracia_balanceada = balanced_accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Definindo as cores
cor_barra = "#fbe2e2"
cor_texto = "#064b85"

# Visualização gráfica da matriz de confusão
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap=sns.light_palette(cor_barra, as_cmap=True),
            annot_kws={"color": cor_texto}, xticklabels=['0 (Sem atraso)', '1 (Com atraso)'],
            yticklabels=['0 (Sem atraso)', '1 (Com atraso)'])
plt.xlabel('Predições', color=cor_texto)
plt.ylabel('Valores Reais', color=cor_texto)
plt.title('Matriz de Confusão', color=cor_texto)
plt.show()

print(f'True Positive: {tp}')
print(f'False Positive: {fp}')
print(f'False Negative: {fn}')
print(f'True Negative: {tn}')
print(f'Accuracy: {accuracy}')
print(f"Acurácia Balanceada: {acuracia_balanceada:.3f}")
print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1 Score: {f1}')

"""## SMOTE apenas com 3 variáveis independentes"""

# Dividindo os dados em conjuntos de treinamento e teste
x = df_logistic_regression[['DAY_OF_WEEK_BINARY', 'CAT_CRS_DEP_TIME_BINARY', 'FLIGHT_ROUTE_BINARY']]  # features = variáveis independentes
y = df_logistic_regression['HAS_DELAY']  # target
# Treino: 70% e teste: 30%
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)
# Aplicar SMOTE apenas nos dados de treinamento
smote = SMOTE(random_state=42)
x_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)
# Criar o modelo de regressão logística
model = LogisticRegression()
# Treinar o modelo com dados balanceados
model.fit(x_train_resampled, y_train_resampled)
# Prever os valores para o conjunto de teste
y_prob = model.predict_proba(x_test)[:, 1]
y_pred = model.predict(x_test)
# Calcular a curva ROC e a área sob a curva (AUC)
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = roc_auc_score(y_test, y_prob)
# Calcular a matriz de confusão
conf_matrix = confusion_matrix(y_test, y_pred)
true_positive = conf_matrix[1, 1]
false_positive = conf_matrix[0, 1]
false_negative = conf_matrix[1, 0]
true_negative = conf_matrix[0, 0]
# Calcular as métricas de desempenho
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
log_loss = log_loss(y_test, y_pred)
# Exibir as métricas de desempenho
print("Matriz de Confusão:")
print(conf_matrix)
print("True Positive:", true_positive)
print("False Positive:", false_positive)
print("False Negative:", false_negative)
print("True Negative:", true_negative)
print("Acurácia:", accuracy)
print("Precisão:", precision)
print("Recall:", recall)
print("F1-score:", f1)
print("AUC ROC:", roc_auc)
print("Log Loss:", log_loss)
# Plotar a curva ROC com cores personalizadas
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='#064B85', label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='#D64550', linestyle='--')
plt.xlabel('False Positive Rate', color='#064B85')
plt.ylabel('True Positive Rate', color='#064B85')
plt.title('Curva ROC', color='#064B85')
plt.legend(loc="lower right", facecolor='#FBE2E2', edgecolor='#064B85', labelcolor='#064B85')
plt.gca().set_facecolor('#FBE2E2')
plt.gca().spines['bottom'].set_color('#064B85')
plt.gca().spines['top'].set_color('#064B85')
plt.gca().spines['right'].set_color('#064B85')
plt.gca().spines['left'].set_color('#064B85')
plt.gca().tick_params(colors='#064B85')
plt.show()
# Plotar o mapa de calor da matriz de confusão com cores personalizadas
plt.figure(figsize=(8, 6))
cmap = sns.color_palette(["#9AD1FF", "#3B94DF", "#0E6BB8", "#064B85"], as_cmap=True)
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap=cmap, xticklabels=['Sem Atraso', 'Atrasado'], yticklabels=['Sem Atraso', 'Atrasado'])
plt.xlabel('Predicted', color='#064B85')
plt.ylabel('True', color='#064B85')
plt.title('Matriz de Confusão', color='#064B85')
plt.gca().set_facecolor('#FBE2E2')
plt.show()

"""## Utilizando ADASYN"""

# Dividindo os dados em conjuntos de treinamento e teste
x = df_logistic_regression[['DAY_OF_WEEK_BINARY', 'CAT_CRS_DEP_TIME_BINARY', 'FLIGHT_ROUTE_BINARY']]  # features = variáveis independentes
y = df_logistic_regression['HAS_DELAY']  # target

# Treino: 70% e teste: 30%
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

# Aplicar ADASYN apenas nos dados de treinamento
adasyn = ADASYN(random_state=42)
x_train_resampled, y_train_resampled = adasyn.fit_resample(x_train, y_train)

# Criar o modelo de regressão logística
model = LogisticRegression()

# Treinar o modelo com dados balanceados
model.fit(x_train_resampled, y_train_resampled)

# Prever os valores para o conjunto de teste
y_prob = model.predict_proba(x_test)[:, 1]
y_pred = model.predict(x_test)

# Calcular a curva ROC e a área sob a curva (AUC)
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = roc_auc_score(y_test, y_prob)

# Calcular a matriz de confusão
conf_matrix = confusion_matrix(y_test, y_pred)
true_positive = conf_matrix[1, 1]
false_positive = conf_matrix[0, 1]
false_negative = conf_matrix[1, 0]
true_negative = conf_matrix[0, 0]

# Calcular as métricas de desempenho
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
log_loss_value = log_loss(y_test, y_prob)

# Exibir as métricas de desempenho
print("Matriz de Confusão:")
print(conf_matrix)
print("True Positive:", true_positive)
print("False Positive:", false_positive)
print("False Negative:", false_negative)
print("True Negative:", true_negative)
print("Acurácia:", accuracy)
print("Precisão:", precision)
print("Recall:", recall)
print("F1-score:", f1)
print("AUC ROC:", roc_auc)
print("Log Loss:", log_loss_value)

# Definir as cores desejadas
cor_barra = "#fbe2e2"
cor_texto = "#064b85"

# Plotar a curva ROC
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color=cor_texto, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color=cor_barra, linestyle='--')
plt.xlabel('False Positive Rate', color=cor_texto)
plt.ylabel('True Positive Rate', color=cor_texto)
plt.title('Curva ROC', color=cor_texto)
plt.legend(loc="lower right")
plt.show()

# Visualização gráfica da matriz de confusão
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=sns.light_palette(cor_barra, as_cmap=True),
            annot_kws={"color": cor_texto}, xticklabels=['Sem Atraso', 'Atrasado'],
            yticklabels=['Sem Atraso', 'Atrasado'])
plt.xlabel('Predições', color=cor_texto)
plt.ylabel('Valores Reais', color=cor_texto)
plt.title('Matriz de Confusão', color=cor_texto)
plt.show()

# Dividindo os dados em conjuntos de treinamento e teste
x = df_logistic_regression[['DAY_OF_WEEK_BINARY', 'CAT_CRS_DEP_TIME_BINARY', 'FLIGHT_ROUTE_BINARY']]  # features = variáveis independentes
y = df_logistic_regression['HAS_DELAY']  # target
# Treino: 70% e teste: 30%
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)
# Aplicar SMOTE apenas nos dados de treinamento
smote = SMOTE(random_state=42)
x_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)
# Criar o modelo de regressão logística
model = LogisticRegression()
# Treinar o modelo com dados balanceados
model.fit(x_train_resampled, y_train_resampled)
# Prever as probabilidades para o conjunto de teste
y_prob = model.predict_proba(x_test)[:, 1]
# Definir os limiares a serem testados
thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
# Armazenar as métricas para cada limiar
results = []
for threshold in thresholds:
    # Calcular previsões binárias com base no limiar
    y_pred_threshold = (y_prob >= threshold).astype(int)
    # Calcular a matriz de confusão
    conf_matrix = confusion_matrix(y_test, y_pred_threshold)
    true_positive = conf_matrix[1, 1]
    false_positive = conf_matrix[0, 1]
    false_negative = conf_matrix[1, 0]
    true_negative = conf_matrix[0, 0]
    # Calcular as métricas de desempenho
    accuracy = accuracy_score(y_test, y_pred_threshold)
    precision = precision_score(y_test, y_pred_threshold, zero_division=0)
    recall = recall_score(y_test, y_pred_threshold)
    f1 = f1_score(y_test, y_pred_threshold)
    # Armazenar os resultados
    results.append({
        'Threshold': threshold,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1': f1,
        'True Positive': true_positive,
        'False Positive': false_positive,
        'False Negative': false_negative,
        'True Negative': true_negative
    })
# Converter os resultados para um DataFrame
results_df = pd.DataFrame(results)
# Exibir os resultados
print(results_df)
# Plotar as métricas de desempenho em função dos limiares
plt.figure(figsize=(10, 6))
plt.plot(results_df['Threshold'], results_df['Accuracy'], label='Accuracy')
plt.plot(results_df['Threshold'], results_df['Precision'], label='Precision')
plt.plot(results_df['Threshold'], results_df['Recall'], label='Recall')
plt.plot(results_df['Threshold'], results_df['F1'], label='F1-score')
plt.xlabel('Threshold')
plt.ylabel('Score')
plt.title('Desempenho em Função do Limiar')
plt.legend()
plt.show()

"""## Baixando o DF"""

#Salvando o DataFrame como um arquivo CSV
# Baixar o arquivo CSV
from google.colab import files
files.download('analise_resultados.csv')